{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "#定义一个基础的神经网络类\n",
    "class neuralNetWork(object):\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            sizes: list 相应的数值代指各层神经元的数量;\n",
    "            weights: 神经元之间的权重\n",
    "            biases: 神经元的偏置\n",
    "            weights和biases参数都以标准正态分布初始化;\n",
    "        \"\"\"\n",
    "        self.sizes = sizes\n",
    "        self.num_layers = len(sizes)\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "        返回神经元的输出，a是输入层的输入数据\n",
    "        \"\"\"\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, train, epochs, min_batch_size, eta, test=None):\n",
    "        \"\"\"\n",
    "        用小批量随机梯度下降算法训练神经网络。\n",
    "        train: list, 每个元素是(x, y)形式的tuple， x代表输入数据特征， y表带输出目标值。\n",
    "        epochs: 训练轮数\n",
    "        min_batch_size: 每轮训练样本数\n",
    "        eta: 学习速率\n",
    "        test: 测试集\n",
    "        \"\"\"\n",
    "        test = list(test)\n",
    "        train = list(train)\n",
    "        if test:\n",
    "            n_test = len(test)\n",
    "        n_train = len(train)\n",
    "        for epoch in range(epochs):\n",
    "            random.shuffle(train)\n",
    "            min_batches = [train[index:index+min_batch_size] for index in range(0, n_train, min_batch_size)]\n",
    "            for mini_batch in min_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(epoch, self.evaluate(test), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} 结束。\".format(epoch))\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        用小批量随机梯度下降和后向传播算法更新参数weights和biases\n",
    "        mini_batch: 小批量数据\n",
    "        eta: 学习速率\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_w, delta_nabla_b = self.backprop(x, y)\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        self.weights = [w - (eta/len(mini_batch))*nw \n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta/len(mini_batch))*nb \n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        x: 单个样本的特征值\n",
    "        y: 单个样本的目标值\n",
    "        return: tuple(nabla_w, nabla_b)\n",
    "        \"\"\"\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        # list to store all the activations, layer by layer\n",
    "        activations = [x]\n",
    "        # list to store all the z vectors, layer by layer\n",
    "        zs = []\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            #print(\"w b的维度分别是：{0}, {1}\".format(w.shape, b.shape))\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # error backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid(zs[-1])\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        nabla_b[-1] = delta\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            nabla_b[-l] = delta\n",
    "        return (nabla_w, nabla_b)\n",
    "    \n",
    "    def evaluate(self, test):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        return: vector,  损失函数C_x对输出层神经元的输出a的偏导数\n",
    "        \"\"\"\n",
    "        return (output_activations - y)\n",
    "    \n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    z: 范围[0, 1], 神经元线下加权值 z = w*x + b\n",
    "    \"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"\n",
    "    return: vector sigmoid函数的导数\n",
    "    \"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "train, validation, test = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "netModel = neuralNetWork([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 6615 / 10000\n",
      "Epoch 1: 8234 / 10000\n",
      "Epoch 2: 8299 / 10000\n",
      "Epoch 3: 8279 / 10000\n",
      "Epoch 4: 8320 / 10000\n",
      "Epoch 5: 8380 / 10000\n",
      "Epoch 6: 8412 / 10000\n",
      "Epoch 7: 8381 / 10000\n",
      "Epoch 8: 8416 / 10000\n",
      "Epoch 9: 8412 / 10000\n",
      "Epoch 10: 8410 / 10000\n",
      "Epoch 11: 8433 / 10000\n",
      "Epoch 12: 8414 / 10000\n",
      "Epoch 13: 8474 / 10000\n",
      "Epoch 14: 8438 / 10000\n",
      "Epoch 15: 8476 / 10000\n",
      "Epoch 16: 8413 / 10000\n",
      "Epoch 17: 8426 / 10000\n",
      "Epoch 18: 8488 / 10000\n",
      "Epoch 19: 8503 / 10000\n",
      "Epoch 20: 8494 / 10000\n",
      "Epoch 21: 8490 / 10000\n",
      "Epoch 22: 8444 / 10000\n",
      "Epoch 23: 8462 / 10000\n",
      "Epoch 24: 8481 / 10000\n",
      "Epoch 25: 8503 / 10000\n",
      "Epoch 26: 8529 / 10000\n",
      "Epoch 27: 8500 / 10000\n",
      "Epoch 28: 8495 / 10000\n",
      "Epoch 29: 8472 / 10000\n"
     ]
    }
   ],
   "source": [
    "netModel.SGD(train, 30, 10, 3.0, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-09e51144b2ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Third-party libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msvm_baseline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Software\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[1;31m# avoid flakes unused variable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Software\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "mnist_svm\n",
    "~~~~~~~~~\n",
    "A classifier program for recognizing handwritten digits from the MNIST\n",
    "data set, using an SVM classifier.\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# My libraries\n",
    "import mnist_loader \n",
    "\n",
    "# Third-party libraries\n",
    "from sklearn import svm\n",
    "\n",
    "def svm_baseline():\n",
    "    training_data, validation_data, test_data = mnist_loader.load_data()\n",
    "    # train\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(training_data[0], training_data[1])\n",
    "    # test\n",
    "    predictions = [int(a) for a in clf.predict(test_data[0])]\n",
    "    num_correct = sum(int(a == y) for a, y in zip(predictions, test_data[1]))\n",
    "    print(\"Baseline classifier using an SVM.\")\n",
    "    print(\"%s of %s values correct.\" % (num_correct, len(test_data[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
